# YouTube-Llama-RAG

A Streamlit app that retrieves YouTube transcripts, cleans and embeds them, and uses Ollama's Llama 3.1 8B model for retrieval-augmented generation Q&A.

## Overview

YouTube-Llama-RAG is a cutting-edge retrieval-augmented generation (RAG) pipeline designed to answer queries based on YouTube video transcripts. The application automatically retrieves the transcript of a specified YouTube video, cleans and organizes the transcript using advanced natural language processing techniques, creates vector embeddings for semantic search, and leverages Ollama's Llama 3.1 8B model (running locally) to generate context-aware and concise responses. All of these features are delivered through an intuitive Streamlit interface.

## Features

- **YouTube Transcript Retrieval:** Automatically extracts transcripts from YouTube videos using the YouTube Transcript API.
- **Data Cleaning & Grouping:** Splits and cleans transcripts into coherent paragraphs via a local NLP pipeline.
- **Semantic Search:** Embeds cleaned transcripts using Sentence Transformers and performs semantic search based on user queries.
- **Context-Aware Q&A:** Generates accurate and concise answers using Ollama's Llama 3.1 8B model.
- **User-Friendly Interface:** Provides an interactive Streamlit app for easy input of YouTube video URLs (or IDs) and queries.

## Installation

1. **Clone the Repository:**
   ```bash
   git clone https://github.com/your-username/YouTube-Llama-RAG.git
   cd YouTube-Llama-RAG
   ```

2. **Create and Activate a Virtual Environment:**
   ```bash
   python -m venv venv
   # On Windows:
   venv\Scripts\activate
   # On macOS/Linux:
   source venv/bin/activate
   ```

3. **Install Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set Up Environment Variables (if needed):**  
   Create a `.env` file in the project root.  
   *Note:* When using Ollama locally, external API keys may not be required.

5. **Install and Configure Ollama:**  
   Ensure you have [Ollama](https://ollama.ai/) installed and that you have the Llama 3.1 8B model available on your local machine. You can test it by running:
   ```bash
   ollama run llama3.1:8b
   ```

## Usage

1. **Launch the Streamlit App:**
   ```bash
   streamlit run app.py
   ```

2. **Interact with the App:**  
   - Enter a YouTube video URL (or just the video ID) in the input field.
   - Enter your query.
   - Click the "Generate Response" button to view the context-aware answer generated by the model.

## Contributing

Contributions are welcome! To contribute:

1. Fork the repository.
2. Create a new branch for your feature or bug fix:
   ```bash
   git checkout -b feature/your-feature-name
   ```
3. Commit your changes:
   ```bash
   git commit -m "Add: description of changes"
   ```
4. Push your branch:
   ```bash
   git push origin feature/your-feature-name
   ```
5. Open a Pull Request on GitHub with a detailed description of your changes.

## License

This project is licensed under the [MIT License](LICENSE).

## Acknowledgments

- [Ollama](https://ollama.ai/) for local model inference.
- [YouTube Transcript API](https://github.com/jdepoix/youtube-transcript-api) for transcript extraction.
- [Sentence Transformers](https://www.sbert.net/) for semantic search and embedding generation.
- [Streamlit](https://streamlit.io/) for creating an intuitive web interface.

---

Feel free to reach out with any questions, suggestions, or feedback!

